#pragma use_dxc
#pragma enable_d3d11_debug_symbols

#pragma kernel CSMain

#include "UnityCG.cginc"
#include "../Common.hlsl"

static const uint ray_sample_per_pixel = 8;
static const float camera_near_z = 0.1;
static const float ray_marching_thickness = 1.0;
static const uint ray_marching_sample_count = 8;
static const float ray_marching_width = 128;

Texture2D<float> _depth_texture;
Texture2D _position_texture;
Texture2D _normal_texture;
Texture2DArray _blue_noise_texture_array;

SamplerState _inline_point_clamp_sampler;
SamplerState _inline_linear_clamp_sampler;
float4 _camera_pixel_size_and_screen_size;
float4x4 _view_projection_matrix;

float4x4 _projection_matrix;
float4x4 _inverse_projection_matrix;

float4x4 _world_to_camera_matrix;
float4x4 _camera_to_world_matrix;
float4x4 _camera_to_normalized_ndc_matrix;
float4 _camera_near_far;
int frame_index;

RWTexture2D<float4> _output_texture;

float4 sample_blue_noise(uint2 screen_pixel_location, uint sample_index)
{
    return _blue_noise_texture_array.Load(uint4(screen_pixel_location % 128, sample_index % 64, 0));
}

float get_linearized_depth(float2 texcoord)
{
    float depth = _depth_texture.SampleLevel(_inline_point_clamp_sampler, texcoord, 0).x;
    return LinearEyeDepth(depth);
}

float4 ndc_position_from_camera_space_position(float3 position)
{
    return mul(_camera_to_normalized_ndc_matrix, float4(position, 1.0));
}

float3 screen_space_position_from_camera_space_position(float3 position)
{
    float4 ndc = ndc_position_from_camera_space_position(position);
    float2 pixel_position = ndc.xy / ndc.w;
    pixel_position *= _camera_pixel_size_and_screen_size.zw;
    return float3(pixel_position, position.z);
}

float3 camera_space_position_from_screen_space_position(float2 uv, float depth_sample)
{
    float3 camera_space_position_derived = compute_view_position_perspectiveLH(depth_sample, uv, _projection_matrix);
    camera_space_position_derived.z *= -1.0;
    return camera_space_position_derived;
}

void find_horizons(inout uint occlusion_bits,
    float3 V, float3 camera_space_position,
    float ang_N, float ang_off, float cos_N,
    float2 ray_start, float2 screen_space_sample_direction,
    float d, float2 XY)
{
    float2 ray_direction = screen_space_sample_direction * d;
    const float step_length = pow(ray_marching_width, 1.0 / ray_marching_sample_count);
    float t0 = pow(step_length, XY.x);
    for (uint i = 0; i < ray_marching_sample_count; i++)
    {
        float t = t0 * pow(step_length, i);
        float2 screen_space_sample_position = ray_start + ray_direction * t;
        if(screen_space_sample_position.x < 0.0 || screen_space_sample_position.x >= _camera_pixel_size_and_screen_size.z || screen_space_sample_position.y < 0.0 || screen_space_sample_position.y >= _camera_pixel_size_and_screen_size.w)
        {
            break;
        }

        float2 screen_sample_uv = (screen_space_sample_position + 0.5) * _camera_pixel_size_and_screen_size.xy;
        float depth_sample = get_linearized_depth(screen_sample_uv);
        float3 camera_space_sample_position = camera_space_position_from_screen_space_position(screen_sample_uv, depth_sample);

        float3 delta_front = camera_space_sample_position - camera_space_position;
        float3 delta_back  = delta_front - V * ray_marching_thickness;

        // project samples onto unit circle and compute angles relative to V
        float2 horizon_cos = float2(dot(normalize(delta_front), V), 
               dot(normalize(delta_back), V));
        float2 horizon_ang = acos_approx_safe(horizon_cos) * d;
        // shift relative angles from V to N + map to [0,1]
        float2 horizon_01 = clamp(horizon_ang * M_1_PI + ang_off, 0.0, 1.0);
        // sampling direction flips min/max angles
        horizon_01 = d >= 0.0 ? horizon_01.xy : horizon_01.yx;
        
        // map to slice relative distribution
        horizon_01.x = slice_rel_cdf_cos(horizon_01.x, ang_N, cos_N, d > 0.0);
        horizon_01.y = slice_rel_cdf_cos(horizon_01.y, ang_N, cos_N, d > 0.0);

        // jitter sample locations + clamp01
        horizon_01 = clamp(horizon_01 + XY.y * (1.0 / 32.0), 0.0, 1.0);
       
        uint occlusion_bitmask; // turn arc into bit mask
        {
            uint2 horInt = uint2(floor(horizon_01 * 32.0));

            uint OxFFFFFFFFu = 0xFFFFFFFFu; // don't inline here! ANGLE bug: https://issues.angleproject.org/issues/353039526

            uint mX = horInt.x < 32u ? OxFFFFFFFFu <<        horInt.x  : 0u;
            uint mY = horInt.y != 0u ? OxFFFFFFFFu >> (32u - horInt.y) : 0u;

            occlusion_bitmask = mX & mY;            
        }

        occlusion_bits = occlusion_bits | occlusion_bitmask;
    }
}

float ground_truth_visibility_mask_ao(uint2 screen_pixel_location, float3 camera_space_position, float3 camera_space_normal)
{
    float3 V = -normalize(camera_space_position);
    float ao = 0.0;
    for (uint sample_index = 0; sample_index < ray_sample_per_pixel; sample_index++)
    {
        uint blue_noise_sample_index = frame_index * ray_sample_per_pixel + sample_index;
        float3 random_number = sample_blue_noise(screen_pixel_location, blue_noise_sample_index).xyz;
        
        // slice direction sampling
        float3 camera_space_sample_direction;
        float2 screen_space_sample_direction;
        {
            float4 quaternion_to_V = quaternion_create(V);
            float4 quaternion_from_V = quaternion_to_V * float4(float3(-1.0, -1.0, -1.0), 1.0); // conjugate
            float3 camera_space_normal_from_V = transform_xyz_by_unit_quaternion_xy0s(camera_space_normal, quaternion_from_V);
            screen_space_sample_direction = sample_slice_direction(camera_space_normal_from_V, random_number.x);
            camera_space_sample_direction = transform_xy0_by_unit_quaternion_xy0s(screen_space_sample_direction, quaternion_from_V);
            float3 ray_start = screen_space_position_from_camera_space_position(camera_space_position);
            float3 ray_end = screen_space_position_from_camera_space_position(camera_space_position + camera_space_sample_direction * camera_near_z * 0.5);
            float3 ray_direction = ray_end - ray_start;
            ray_direction /= length(ray_direction.xy);
            screen_space_sample_direction = ray_direction.xy;
        }

        // construct slice
        float cos_N;
        float ang_N;
        {
            float3 slice_N = cross(V, camera_space_sample_direction);
            float3 proj_N = camera_space_normal - slice_N * dot(camera_space_normal, slice_N);
            float proj_N_squared_length = dot(proj_N, proj_N);
            if (proj_N_squared_length == 0.0)
            {
                return 1.0;
            }
            float proj_N_rcp_len = rsqrt(proj_N_squared_length);
            cos_N = dot(proj_N, V) * proj_N_rcp_len;
            float3 T = cross(slice_N, proj_N);
            float sgn = dot(V, T) < 0.0 ? -1.0 : 1.0;
            ang_N = sgn * acos_approx_safe(cos_N);
        }
        
        // find horizons
        float2 ray_start = screen_space_position_from_camera_space_position(camera_space_position).xy;
        float ang_off = ang_N * M_1_PI + 0.5;
        uint occlusion_bits = 0u;
        find_horizons(occlusion_bits, V, camera_space_position, ang_N, ang_off, cos_N, ray_start, screen_space_sample_direction, -1.0, random_number.yz);
        random_number.y = 1.0 - random_number.y;
        find_horizons(occlusion_bits, V, camera_space_position, ang_N, ang_off, cos_N, ray_start, screen_space_sample_direction, 1.0, random_number.yz);
        
        float occ0 = float(countbits(occlusion_bits)) * (1.0 / 32.0);
        float slice_weight = 1.0;
        ao += slice_weight - slice_weight * occ0;
    }
    ao /= float(ray_sample_per_pixel);
    return ao;
}

float reference_ssao(uint2 screen_pixel_location, float3 camera_space_position, float3 camera_space_normal)
{
    float occlusion = 0.0;
    for (uint sample_index = 0; sample_index < ray_sample_per_pixel; sample_index++)
    {
        uint blue_noise_sample_index = frame_index * ray_sample_per_pixel + sample_index;
        float3 random_number = sample_blue_noise(screen_pixel_location, blue_noise_sample_index).xyz;
        
        float3 sample_direction = sample_sphere(random_number.xy);
        // cosine weighted hemisphere
        sample_direction = normalize(sample_direction + camera_space_normal);
        float4 h0 = ndc_position_from_camera_space_position(camera_space_position);
        float4 h1 = ndc_position_from_camera_space_position(camera_space_position + sample_direction * camera_near_z * 0.5);
        
        float k0 = 1.0 / h0.w;
        float k1 = 1.0 / h1.w;
        
        float2 p0 = h0.xy * k0;
        float2 p1 = h1.xy * k1;
        
        float2 dp = p1 - p0;
        float dk = k1 - k0;
            
        float2 derivative_of_p_with_respect_to_uv = dp * _camera_pixel_size_and_screen_size.zw;
        float one_over_length_of_derivative_of_p_with_respect_to_uv = rsqrt(dot(derivative_of_p_with_respect_to_uv, derivative_of_p_with_respect_to_uv));

        // 1 px step size
        dp = dp * one_over_length_of_derivative_of_p_with_respect_to_uv;
        dk = dk * one_over_length_of_derivative_of_p_with_respect_to_uv;
            
        float random_sample = random_number.z;
        
        const float step_size = pow(ray_marching_width, 1.0 / ray_marching_sample_count);
            
        float t = pow(step_size, random_sample);// init t: [1, s]

        for (float i = 0.0; i < ray_marching_sample_count; i++)
        {
            float2 uv = p0 + dp * t;
            if (any(uv < 0) || any(uv >= 1))
            {
                break;
            }
            float k = k0 + dk * t;

            // exponential step size
            t *= step_size;

            float estimated_depth = 1.0 / k;
            float depth_sample = get_linearized_depth(uv);
            if((estimated_depth > depth_sample) && (estimated_depth < (depth_sample + ray_marching_thickness)))
            {
                occlusion += 1.0;
                break;
            }
        }
    }
    occlusion /= float(ray_sample_per_pixel);
    return 1.0 - occlusion;
}

[numthreads(8,8,1)]
void CSMain (uint2 id : SV_DispatchThreadID)
{
    if (any(id.xy >= _camera_pixel_size_and_screen_size.zw))
    {
        return;
    }
    
    float2 uv = (id.xy + 0.5) / _camera_pixel_size_and_screen_size.zw;
    
    float3 world_space_position = _position_texture.SampleLevel(_inline_point_clamp_sampler, uv, 0).xyz;
    float3 camera_space_position = mul(_world_to_camera_matrix, float4(world_space_position, 1.0)).xyz;
    
    float3 world_space_normal = _normal_texture.SampleLevel(_inline_point_clamp_sampler, uv, 0).xyz;
    float3 camera_space_normal = normalize(mul(transpose((float3x3)_camera_to_world_matrix), world_space_normal).xyz);

    #if 0
    float ao = reference_ssao(id.xy, camera_space_position, camera_space_normal);
    #else 
    float ao = ground_truth_visibility_mask_ao(id.xy, camera_space_position, camera_space_normal);
    #endif
    
    _output_texture[id.xy] = float4(ao, ao, ao, 0.0);
}
